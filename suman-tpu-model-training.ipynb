{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.15","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":71549,"databundleVersionId":8561470,"sourceType":"competition"},{"sourceId":8236972,"sourceType":"datasetVersion","datasetId":4885707},{"sourceId":8942685,"sourceType":"datasetVersion","datasetId":5380929},{"sourceId":9137883,"sourceType":"datasetVersion","datasetId":5518497}],"dockerImageVersionId":30777,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install polars --no-index --find-links=file:///kaggle/input/polars-and-duckdb/kaggle/working/mysitepackages/polars_pkg","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-01T15:12:04.373046Z","iopub.execute_input":"2024-10-01T15:12:04.374184Z","iopub.status.idle":"2024-10-01T15:12:09.344647Z","shell.execute_reply.started":"2024-10-01T15:12:04.374142Z","shell.execute_reply":"2024-10-01T15:12:09.343365Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Looking in links: file:///kaggle/input/polars-and-duckdb/kaggle/working/mysitepackages/polars_pkg\nProcessing /kaggle/input/polars-and-duckdb/kaggle/working/mysitepackages/polars_pkg/polars-0.20.16-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nInstalling collected packages: polars\nSuccessfully installed polars-0.20.16\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport glob\nimport os\nimport polars as pl\n#import duckdb as dd\n#from tqdm import tqdm\nimport matplotlib.pyplot as plt\n#import cv2\n#from pydicom import dcmread\nimport warnings\n#from sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss\nimport pickle\n#import gc\nimport ctypes\nimport tensorflow as tf\nimport tensorflow_io as tfio\nfrom tensorflow import keras","metadata":{"execution":{"iopub.status.busy":"2024-10-01T15:12:41.938637Z","iopub.execute_input":"2024-10-01T15:12:41.939716Z","iopub.status.idle":"2024-10-01T15:12:59.998916Z","shell.execute_reply.started":"2024-10-01T15:12:41.939675Z","shell.execute_reply":"2024-10-01T15:12:59.997901Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"WARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1727795573.061531      13 common_lib.cc:798] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: ===\nlearning/45eac/tfrc/runtime/common_lib.cc:479\nD1001 15:12:53.069946221      13 config.cc:196]                        gRPC EXPERIMENT call_status_override_on_cancellation   OFF (default:OFF)\nD1001 15:12:53.069962677      13 config.cc:196]                        gRPC EXPERIMENT call_v3                                OFF (default:OFF)\nD1001 15:12:53.069965918      13 config.cc:196]                        gRPC EXPERIMENT canary_client_privacy                  ON  (default:ON)\nD1001 15:12:53.069968389      13 config.cc:196]                        gRPC EXPERIMENT capture_base_context                   ON  (default:ON)\nD1001 15:12:53.069971324      13 config.cc:196]                        gRPC EXPERIMENT client_idleness                        ON  (default:ON)\nD1001 15:12:53.069973699      13 config.cc:196]                        gRPC EXPERIMENT client_privacy                         ON  (default:ON)\nD1001 15:12:53.069976011      13 config.cc:196]                        gRPC EXPERIMENT dapper_request_wire_size               OFF (default:OFF)\nD1001 15:12:53.069978251      13 config.cc:196]                        gRPC EXPERIMENT empty_experiment                       OFF (default:OFF)\nD1001 15:12:53.069980451      13 config.cc:196]                        gRPC EXPERIMENT event_engine_client                    OFF (default:OFF)\nD1001 15:12:53.069982606      13 config.cc:196]                        gRPC EXPERIMENT event_engine_dns                       ON  (default:ON)\nD1001 15:12:53.069984818      13 config.cc:196]                        gRPC EXPERIMENT event_engine_listener                  ON  (default:ON)\nD1001 15:12:53.069987035      13 config.cc:196]                        gRPC EXPERIMENT free_large_allocator                   OFF (default:OFF)\nD1001 15:12:53.069989191      13 config.cc:196]                        gRPC EXPERIMENT google_no_envelope_resolver            OFF (default:OFF)\nD1001 15:12:53.069991362      13 config.cc:196]                        gRPC EXPERIMENT http2_stats_fix                        OFF (default:OFF)\nD1001 15:12:53.069993537      13 config.cc:196]                        gRPC EXPERIMENT keepalive_fix                          OFF (default:OFF)\nD1001 15:12:53.069995738      13 config.cc:196]                        gRPC EXPERIMENT keepalive_server_fix                   ON  (default:ON)\nD1001 15:12:53.069998084      13 config.cc:196]                        gRPC EXPERIMENT loas_do_not_prefer_rekey_next_protocol OFF (default:OFF)\nD1001 15:12:53.070000308      13 config.cc:196]                        gRPC EXPERIMENT loas_prod_to_cloud_prefer_pfs_ciphers  OFF (default:OFF)\nD1001 15:12:53.070002541      13 config.cc:196]                        gRPC EXPERIMENT monitoring_experiment                  ON  (default:ON)\nD1001 15:12:53.070004773      13 config.cc:196]                        gRPC EXPERIMENT multiping                              OFF (default:OFF)\nD1001 15:12:53.070006976      13 config.cc:196]                        gRPC EXPERIMENT peer_state_based_framing               OFF (default:OFF)\nD1001 15:12:53.070009174      13 config.cc:196]                        gRPC EXPERIMENT pending_queue_cap                      ON  (default:ON)\nD1001 15:12:53.070011446      13 config.cc:196]                        gRPC EXPERIMENT pick_first_happy_eyeballs              ON  (default:ON)\nD1001 15:12:53.070013670      13 config.cc:196]                        gRPC EXPERIMENT promise_based_client_call              OFF (default:OFF)\nD1001 15:12:53.070015791      13 config.cc:196]                        gRPC EXPERIMENT promise_based_inproc_transport         OFF (default:OFF)\nD1001 15:12:53.070017938      13 config.cc:196]                        gRPC EXPERIMENT promise_based_server_call              OFF (default:OFF)\nD1001 15:12:53.070020211      13 config.cc:196]                        gRPC EXPERIMENT registered_method_lookup_in_transport  ON  (default:ON)\nD1001 15:12:53.070022401      13 config.cc:196]                        gRPC EXPERIMENT rfc_max_concurrent_streams             ON  (default:ON)\nD1001 15:12:53.070024695      13 config.cc:196]                        gRPC EXPERIMENT round_robin_delegate_to_pick_first     ON  (default:ON)\nD1001 15:12:53.070028237      13 config.cc:196]                        gRPC EXPERIMENT rstpit                                 OFF (default:OFF)\nD1001 15:12:53.070030574      13 config.cc:196]                        gRPC EXPERIMENT schedule_cancellation_over_write       OFF (default:OFF)\nD1001 15:12:53.070032943      13 config.cc:196]                        gRPC EXPERIMENT server_privacy                         ON  (default:ON)\nD1001 15:12:53.070035721      13 config.cc:196]                        gRPC EXPERIMENT tcp_frame_size_tuning                  OFF (default:OFF)\nD1001 15:12:53.070037938      13 config.cc:196]                        gRPC EXPERIMENT tcp_rcv_lowat                          OFF (default:OFF)\nD1001 15:12:53.070040105      13 config.cc:196]                        gRPC EXPERIMENT trace_record_callops                   OFF (default:OFF)\nD1001 15:12:53.070042261      13 config.cc:196]                        gRPC EXPERIMENT unconstrained_max_quota_buffer_size    OFF (default:OFF)\nD1001 15:12:53.070044399      13 config.cc:196]                        gRPC EXPERIMENT v3_backend_metric_filter               OFF (default:OFF)\nD1001 15:12:53.070046564      13 config.cc:196]                        gRPC EXPERIMENT v3_channel_idle_filters                ON  (default:ON)\nD1001 15:12:53.070048948      13 config.cc:196]                        gRPC EXPERIMENT v3_compression_filter                  ON  (default:ON)\nD1001 15:12:53.070051155      13 config.cc:196]                        gRPC EXPERIMENT v3_server_auth_filter                  OFF (default:OFF)\nD1001 15:12:53.070053314      13 config.cc:196]                        gRPC EXPERIMENT work_serializer_clears_time_cache      OFF (default:OFF)\nD1001 15:12:53.070055414      13 config.cc:196]                        gRPC EXPERIMENT work_serializer_dispatch               OFF (default:OFF)\nD1001 15:12:53.070057596      13 config.cc:196]                        gRPC EXPERIMENT write_size_cap                         ON  (default:ON)\nD1001 15:12:53.070059813      13 config.cc:196]                        gRPC EXPERIMENT write_size_policy                      ON  (default:ON)\nD1001 15:12:53.070062066      13 config.cc:196]                        gRPC EXPERIMENT wrr_delegate_to_pick_first             ON  (default:ON)\nI1001 15:12:53.070247208      13 ev_epoll1_linux.cc:123]               grpc epoll fd: 60\nD1001 15:12:53.070259532      13 ev_posix.cc:113]                      Using polling engine: epoll1\nD1001 15:12:53.080629187      13 lb_policy_registry.cc:46]             registering LB policy factory for \"priority_experimental\"\nD1001 15:12:53.080639910      13 lb_policy_registry.cc:46]             registering LB policy factory for \"outlier_detection_experimental\"\nD1001 15:12:53.080647619      13 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_target_experimental\"\nD1001 15:12:53.080650764      13 lb_policy_registry.cc:46]             registering LB policy factory for \"pick_first\"\nD1001 15:12:53.080653882      13 lb_policy_registry.cc:46]             registering LB policy factory for \"round_robin\"\nD1001 15:12:53.080656816      13 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_round_robin\"\nD1001 15:12:53.080684456      13 lb_policy_registry.cc:46]             registering LB policy factory for \"grpclb\"\nD1001 15:12:53.080702435      13 dns_resolver_plugin.cc:43]            Using EventEngine dns resolver\nD1001 15:12:53.080720495      13 lb_policy_registry.cc:46]             registering LB policy factory for \"rls_experimental\"\nD1001 15:12:53.080759402      13 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_manager_experimental\"\nD1001 15:12:53.080767200      13 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_impl_experimental\"\nD1001 15:12:53.080770205      13 lb_policy_registry.cc:46]             registering LB policy factory for \"cds_experimental\"\nD1001 15:12:53.080774024      13 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_override_host_experimental\"\nD1001 15:12:53.080777329      13 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_wrr_locality_experimental\"\nD1001 15:12:53.080780251      13 lb_policy_registry.cc:46]             registering LB policy factory for \"ring_hash_experimental\"\nD1001 15:12:53.080783511      13 certificate_provider_registry.cc:33]  registering certificate provider factory for \"file_watcher\"\nD1001 15:12:53.080812106      13 channel_init.cc:157]                  Filter server-auth not registered, but is referenced in the after clause of grpc-server-authz when building channel stack SERVER_CHANNEL\nI1001 15:12:53.082713015      13 ev_epoll1_linux.cc:359]               grpc epoll fd: 62\nI1001 15:12:53.100422789      13 tcp_socket_utils.cc:689]              Disabling AF_INET6 sockets because ::1 is not available.\nI1001 15:12:53.104055062     173 socket_utils_common_posix.cc:452]     Disabling AF_INET6 sockets because ::1 is not available.\nI1001 15:12:53.104101437     173 socket_utils_common_posix.cc:379]     TCP_USER_TIMEOUT is available. TCP_USER_TIMEOUT will be used thereafter\nE1001 15:12:53.109619031     171 oauth2_credentials.cc:238]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {created_time:\"2024-10-01T15:12:53.109598484+00:00\", grpc_status:2}\n","output_type":"stream"}]},{"cell_type":"code","source":"\"\"\"print('Running on TPU ', TPU.master())\nexcept ValueError:\n    print('Running on GPU')\n    TPU = None\n\nif TPU:\n    IS_TPU = True\n    tf.config.experimental_connect_to_cluster(TPU)\"\"\"\n\n# detect TPUs\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='local')\ntf.tpu.experimental.initialize_tpu_system(tpu)\ntpu_strategy = tf.distribute.TPUStrategy(tpu)\n\nprint(\"Number of accelerators: \", tpu_strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T15:13:25.956098Z","iopub.execute_input":"2024-10-01T15:13:25.957036Z","iopub.status.idle":"2024-10-01T15:13:34.661277Z","shell.execute_reply.started":"2024-10-01T15:13:25.957001Z","shell.execute_reply":"2024-10-01T15:13:34.660223Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\nINFO:tensorflow:Initializing the TPU system: local\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1727795610.127794      13 service.cc:145] XLA service 0x55de2218f340 initialized for platform TPU (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1727795610.127856      13 service.cc:153]   StreamExecutor device (0): TPU, 2a886c8\nI0000 00:00:1727795610.127861      13 service.cc:153]   StreamExecutor device (1): TPU, 2a886c8\nI0000 00:00:1727795610.127864      13 service.cc:153]   StreamExecutor device (2): TPU, 2a886c8\nI0000 00:00:1727795610.127867      13 service.cc:153]   StreamExecutor device (3): TPU, 2a886c8\nI0000 00:00:1727795610.127869      13 service.cc:153]   StreamExecutor device (4): TPU, 2a886c8\nI0000 00:00:1727795610.127872      13 service.cc:153]   StreamExecutor device (5): TPU, 2a886c8\nI0000 00:00:1727795610.127874      13 service.cc:153]   StreamExecutor device (6): TPU, 2a886c8\nI0000 00:00:1727795610.127877      13 service.cc:153]   StreamExecutor device (7): TPU, 2a886c8\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:Finished initializing TPU system.\nINFO:tensorflow:Found TPU system:\nINFO:tensorflow:*** Num TPU Cores: 8\nINFO:tensorflow:*** Num TPU Workers: 1\nINFO:tensorflow:*** Num TPU Cores Per Worker: 8\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:4, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:5, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:6, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:7, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\nNumber of accelerators:  8\n","output_type":"stream"}]},{"cell_type":"code","source":"def read_and_parse_dicom_files_tensorflow_train(full_file_path):\n    #tf.config.run_functions_eagerly(True)\n    raw_image = tf.io.read_file(full_file_path)\n    sp = tf.strings.split(tf.gather(tf.strings.split(full_file_path, 'images/'), 1), '/')\n    N = tf.size(sp)\n    LEN = tf.strings.length(tf.gather(sp, 0))+tf.strings.length(tf.gather(sp, 2))\n    \n    # Add missing file metadata to avoid warnnigs flooding\n    if   LEN==12: raw_image = tf.strings.regex_replace(raw_image, pattern=b'DICM\\x02\\x00\\x01\\x00', rewrite=b'DICM\\x02\\x00\\x00\\x00UL\\x04\\x00\\x92\\x00\\x00\\x00\\x02\\x00\\x01\\x00')\n    elif LEN==13: raw_image = tf.strings.regex_replace(raw_image, pattern=b'DICM\\x02\\x00\\x01\\x00', rewrite=b'DICM\\x02\\x00\\x00\\x00UL\\x04\\x00\\x92\\x00\\x00\\x00\\x02\\x00\\x01\\x00')\n    elif LEN==14: raw_image = tf.strings.regex_replace(raw_image, pattern=b'DICM\\x02\\x00\\x01\\x00', rewrite=b'DICM\\x02\\x00\\x00\\x00UL\\x04\\x00\\x94\\x00\\x00\\x00\\x02\\x00\\x01\\x00')\n    elif LEN==15: raw_image = tf.strings.regex_replace(raw_image, pattern=b'DICM\\x02\\x00\\x01\\x00', rewrite=b'DICM\\x02\\x00\\x00\\x00UL\\x04\\x00\\x94\\x00\\x00\\x00\\x02\\x00\\x01\\x00')\n    elif LEN==16: raw_image = tf.strings.regex_replace(raw_image, pattern=b'DICM\\x02\\x00\\x01\\x00', rewrite=b'DICM\\x02\\x00\\x00\\x00UL\\x04\\x00\\x96\\x00\\x00\\x00\\x02\\x00\\x01\\x00')\n    elif LEN==17: raw_image = tf.strings.regex_replace(raw_image, pattern=b'DICM\\x02\\x00\\x01\\x00', rewrite=b'DICM\\x02\\x00\\x00\\x00UL\\x04\\x00\\x96\\x00\\x00\\x00\\x02\\x00\\x01\\x00')\n    elif LEN==18: raw_image = tf.strings.regex_replace(raw_image, pattern=b'DICM\\x02\\x00\\x01\\x00', rewrite=b'DICM\\x02\\x00\\x00\\x00UL\\x04\\x00\\x98\\x00\\x00\\x00\\x02\\x00\\x01\\x00')\n    \n    image = tfio.image.decode_dicom_image(raw_image, scale='auto', dtype=tf.float32)\n    m, M=tf.math.reduce_min(image), tf.math.reduce_max(image)\n    image = (tf.image.grayscale_to_rgb(image)-m)/(M-m)\n    image = tf.image.resize(image, (128,128))\n    sqzd_image = tf.squeeze(image)\n    #shaped_image = sqzd_image.set_shape(sqzd_image.get_shape())\n    return sqzd_image\n\ndef read_parse_n_augment_dicom_files(full_file_path):\n    #tf.config.run_functions_eagerly(True)\n    raw_image = tf.io.read_file(full_file_path)\n    sp = tf.strings.split(tf.gather(tf.strings.split(full_file_path, 'images/'), 1), '/')\n    N = tf.size(sp)\n    LEN = tf.strings.length(tf.gather(sp, 0))+tf.strings.length(tf.gather(sp, 2))\n    \n    # Add missing file metadata to avoid warnnigs flooding\n    if   LEN==12: raw_image = tf.strings.regex_replace(raw_image, pattern=b'DICM\\x02\\x00\\x01\\x00', rewrite=b'DICM\\x02\\x00\\x00\\x00UL\\x04\\x00\\x92\\x00\\x00\\x00\\x02\\x00\\x01\\x00')\n    elif LEN==13: raw_image = tf.strings.regex_replace(raw_image, pattern=b'DICM\\x02\\x00\\x01\\x00', rewrite=b'DICM\\x02\\x00\\x00\\x00UL\\x04\\x00\\x92\\x00\\x00\\x00\\x02\\x00\\x01\\x00')\n    elif LEN==14: raw_image = tf.strings.regex_replace(raw_image, pattern=b'DICM\\x02\\x00\\x01\\x00', rewrite=b'DICM\\x02\\x00\\x00\\x00UL\\x04\\x00\\x94\\x00\\x00\\x00\\x02\\x00\\x01\\x00')\n    elif LEN==15: raw_image = tf.strings.regex_replace(raw_image, pattern=b'DICM\\x02\\x00\\x01\\x00', rewrite=b'DICM\\x02\\x00\\x00\\x00UL\\x04\\x00\\x94\\x00\\x00\\x00\\x02\\x00\\x01\\x00')\n    elif LEN==16: raw_image = tf.strings.regex_replace(raw_image, pattern=b'DICM\\x02\\x00\\x01\\x00', rewrite=b'DICM\\x02\\x00\\x00\\x00UL\\x04\\x00\\x96\\x00\\x00\\x00\\x02\\x00\\x01\\x00')\n    elif LEN==17: raw_image = tf.strings.regex_replace(raw_image, pattern=b'DICM\\x02\\x00\\x01\\x00', rewrite=b'DICM\\x02\\x00\\x00\\x00UL\\x04\\x00\\x96\\x00\\x00\\x00\\x02\\x00\\x01\\x00')\n    elif LEN==18: raw_image = tf.strings.regex_replace(raw_image, pattern=b'DICM\\x02\\x00\\x01\\x00', rewrite=b'DICM\\x02\\x00\\x00\\x00UL\\x04\\x00\\x98\\x00\\x00\\x00\\x02\\x00\\x01\\x00')\n    \n    image = tfio.image.decode_dicom_image(raw_image, scale='auto', dtype=tf.float32)\n    m, M=tf.math.reduce_min(image), tf.math.reduce_max(image)\n    image = (tf.image.grayscale_to_rgb(image)-m)/(M-m)\n    \n    brt_image = tf.image.adjust_brightness(image, delta=0.3)\n    contrast_img = tf.image.adjust_contrast(brt_image, contrast_factor=2.0)\n    contrast_img_resized = tf.image.resize(contrast_img, (128,128))\n\n    return tf.squeeze(contrast_img_resized)\n\ndef preprocessing_aug_image(img_path):\n    train_img = read_parse_n_augment_dicom_files(img_path)\n    train_img = tf.reshape(train_img, shape=(128, 128, 3))\n    return train_img\n\ndef load_aug_dataset(image_path, labels):\n    image = preprocessing_aug_image(image_path)\n    return {\"images\": tf.cast(image, tf.float32), \"labels\": tf.cast(labels, tf.float32)}\n\ndef preprocessing(img_path):\n    train_img = read_and_parse_dicom_files_tensorflow_train(img_path)\n    train_img = tf.reshape(train_img, shape=(128, 128, 3))\n    return train_img\n\ndef load_dataset_tensorflow_train(image_path, labels):\n    image = preprocessing(image_path)\n    return {\"images\": tf.cast(image, tf.float32), \"labels\": tf.cast(labels, tf.float32)}\n\ndef dict_to_tuple(inputs):\n    return inputs[\"images\"], inputs[\"labels\"]","metadata":{"execution":{"iopub.status.busy":"2024-10-01T15:20:00.136287Z","iopub.execute_input":"2024-10-01T15:20:00.136703Z","iopub.status.idle":"2024-10-01T15:20:00.154189Z","shell.execute_reply.started":"2024-10-01T15:20:00.136672Z","shell.execute_reply":"2024-10-01T15:20:00.153172Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"condition_for_training = 'right_neural_foraminal_narrowing'\nvertebrae_position = 'l1_l2'\n\nmetadata_file_path = '/kaggle/input/right-neural-foraminal-narrowing-metadata/{0}_{1}_feature_metadata.csv'.format(condition_for_training, vertebrae_position)\nmetadata_df = pl.read_csv(metadata_file_path, low_memory=True)\n\nmetadata_df_severe = metadata_df.filter(pl.col('encoded_severity')==2)\nmetadata_df_moderate = metadata_df.filter(pl.col('encoded_severity')==0)\n\nx_train, x_test_val = train_test_split(metadata_df, test_size=0.4, random_state=42)\nx_test, x_valid = train_test_split(x_test_val, test_size=0.2, random_state=42)\n\nprint(\"Training data shape : {0}\".format(x_train.shape))\nprint(\"Test data shape : {0}\".format(x_test.shape))\nprint(\"Validation data shape : {0}\".format(x_valid.shape))","metadata":{"execution":{"iopub.status.busy":"2024-10-01T15:16:58.713470Z","iopub.execute_input":"2024-10-01T15:16:58.713977Z","iopub.status.idle":"2024-10-01T15:16:59.092565Z","shell.execute_reply.started":"2024-10-01T15:16:58.713944Z","shell.execute_reply":"2024-10-01T15:16:59.091400Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Training data shape : (88000, 3)\nTest data shape : (46934, 3)\nValidation data shape : (11734, 3)\n","output_type":"stream"}]},{"cell_type":"code","source":"def generate_tf_datasets(p_train_df, p_test_df, p_valid_df, p_BATCH_SIZE_PER_REPLICA):\n\n    BATCH_SIZE = p_BATCH_SIZE_PER_REPLICA * tpu_strategy.num_replicas_in_sync\n    \n    train_image_filenames = pl.Series(p_train_df.select(pl.col('full_img_path'))).to_list()\n    train_image_labels = pl.Series(p_train_df.select(pl.col('encoded_severity'))).to_list()\n\n    test_image_filenames = pl.Series(p_test_df.select(pl.col('full_img_path'))).to_list()\n    test_image_labels = pl.Series(p_test_df.select(pl.col('encoded_severity'))).to_list()\n\n    valid_image_filenames = pl.Series(p_valid_df.select(pl.col('full_img_path'))).to_list()\n    valid_image_labels = pl.Series(p_valid_df.select(pl.col('encoded_severity'))).to_list()\n    \n    \n    train_dataset = tf.data.Dataset.from_tensor_slices((train_image_filenames, train_image_labels))\n    test_dataset = tf.data.Dataset.from_tensor_slices((test_image_filenames, test_image_labels))\n    valid_dataset = tf.data.Dataset.from_tensor_slices((valid_image_filenames, valid_image_labels))\n    \n    train_ds = train_dataset.map(load_dataset_tensorflow_train, num_parallel_calls=tf.data.AUTOTUNE)\n    train_ds = train_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n    train_ds = train_ds.batch(batch_size=BATCH_SIZE, drop_remainder=True)\n    train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n\n    test_ds = test_dataset.map(load_dataset_tensorflow_train, num_parallel_calls=tf.data.AUTOTUNE)\n    test_ds = test_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n    test_ds = test_ds.batch(batch_size=BATCH_SIZE, drop_remainder=True)\n    test_ds = test_ds.prefetch(tf.data.AUTOTUNE)\n\n    valid_ds = valid_dataset.map(load_dataset_tensorflow_train, num_parallel_calls=tf.data.AUTOTUNE)\n    valid_ds = valid_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n    valid_ds = valid_ds.batch(batch_size=BATCH_SIZE, drop_remainder=True)\n    valid_ds = valid_ds.prefetch(tf.data.AUTOTUNE)\n    \n    return train_ds, test_ds, valid_ds","metadata":{"execution":{"iopub.status.busy":"2024-10-01T15:17:06.251836Z","iopub.execute_input":"2024-10-01T15:17:06.252394Z","iopub.status.idle":"2024-10-01T15:17:06.261499Z","shell.execute_reply.started":"2024-10-01T15:17:06.252363Z","shell.execute_reply":"2024-10-01T15:17:06.260586Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_ds, test_ds, valid_ds = generate_tf_datasets(p_train_df=x_train, p_test_df=x_test, p_valid_df=x_valid\n                                                   , p_BATCH_SIZE_PER_REPLICA = 16)\n\nBATCH_SIZE = 16 * tpu_strategy.num_replicas_in_sync\n\nmetadata_df_severe_filenames = pl.Series(metadata_df_severe.select(pl.col('full_img_path'))).to_list()\nmetadata_df_severe_labels = pl.Series(metadata_df_severe.select(pl.col('encoded_severity'))).to_list()\n\nmetadata_df_severe_dataset = tf.data.Dataset.from_tensor_slices((metadata_df_severe_filenames, metadata_df_severe_labels))\n\nmetadata_df_severe_ds = metadata_df_severe_dataset.map(load_aug_dataset, num_parallel_calls=tf.data.AUTOTUNE)\nmetadata_df_severe_ds = metadata_df_severe_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\nmetadata_df_severe_ds = metadata_df_severe_ds.batch(batch_size=BATCH_SIZE, drop_remainder=True)\nmetadata_df_severe_ds = metadata_df_severe_ds.prefetch(tf.data.AUTOTUNE)\n\n\ncombined_train_dataset = train_ds.concatenate(metadata_df_severe_ds)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T15:17:09.571732Z","iopub.execute_input":"2024-10-01T15:17:09.572397Z","iopub.status.idle":"2024-10-01T15:17:12.536783Z","shell.execute_reply.started":"2024-10-01T15:17:09.572364Z","shell.execute_reply":"2024-10-01T15:17:12.535795Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras import layers\n\nwith tpu_strategy.scope():\n    \n    rsna_input = layers.Input(shape=(128,128,3), name=\"rsna_input\")\n\n    conv_base = EfficientNetB0(include_top=False, weights=\"imagenet\", input_tensor=rsna_input)\n    conv_base.trainable = False\n\n    max_pool_layer_0 = layers.MaxPooling2D(name=\"max_pool_0\", pool_size=(2, 2), strides=(1, 1), padding=\"same\")(conv_base.output)\n    max_pool_layer_0 = layers.BatchNormalization()(max_pool_layer_0)\n\n    conv2d_1 = layers.Conv2D(filters=100, kernel_size=4, strides=1, padding=\"same\", activation=\"relu\")(max_pool_layer_0)\n    max_pool_layer_1 = layers.MaxPooling2D(name=\"max_pool_1\", pool_size=(2, 2), strides=(1, 1), padding=\"same\")(conv2d_1)\n    max_pool_layer_1 = layers.BatchNormalization()(max_pool_layer_1)\n\n    flattened_layer = layers.Flatten()(max_pool_layer_1)\n\n    hidden_layer1 = layers.Dense(200, activation=\"selu\", kernel_initializer=keras.initializers.LecunNormal(seed=None))(flattened_layer)\n    hidden_layer1 = layers.BatchNormalization()(hidden_layer1)\n    hidden_layer2 = layers.Dense(100, activation=\"selu\", kernel_initializer=keras.initializers.LecunNormal(seed=None))(hidden_layer1)\n    hidden_layer2 = layers.BatchNormalization()(hidden_layer2)\n    hidden_layer3 = layers.Dense(50, activation=\"selu\", kernel_initializer=keras.initializers.LecunNormal(seed=None))(hidden_layer2)\n    hidden_layer3 = layers.BatchNormalization()(hidden_layer3)\n    rsna_output = layers.Dense(3, activation=\"softmax\")(hidden_layer3)\n    model = tf.keras.Model(rsna_input, rsna_output)\n\n    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"keras_effnet_{0}_{1}.keras\".format(condition_for_training, vertebrae_position))\n    early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n    \"\"\"lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-4, decay_steps=1000, decay_rate=0.9)\n    adamax_optimizer = tf.keras.optimizers.Adamax(learning_rate=lr_schedule)\"\"\"\n\n    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adamax\", metrics=[\"accuracy\"])\n    # model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=adamax_optimizer, metrics=[\"accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2024-10-01T15:17:21.937544Z","iopub.execute_input":"2024-10-01T15:17:21.937898Z","iopub.status.idle":"2024-10-01T15:17:41.680184Z","shell.execute_reply.started":"2024-10-01T15:17:21.937870Z","shell.execute_reply":"2024-10-01T15:17:41.679041Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"I0000 00:00:1727795842.006689      13 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"with tpu_strategy.scope():\n    tf.config.run_functions_eagerly(False)\n    history = model.fit(combined_train_dataset, class_weight={0:2, 1:1, 2:4}, epochs=11, validation_data=valid_ds\n                        , callbacks=[checkpoint_cb, early_stopping_cb])","metadata":{"execution":{"iopub.status.busy":"2024-10-01T15:20:24.040679Z","iopub.execute_input":"2024-10-01T15:20:24.041855Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/11\n","output_type":"stream"},{"name":"stderr","text":"2024-10-01 15:20:32.168870: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node StatefulPartitionedCall.\nI0000 00:00:1727796033.032083     836 tpu_compilation_cache_interface.cc:441] TPU host compilation cache miss: cache_key(930313c919672ecc:0:0), session_name()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m  1/695\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:04:07\u001b[0m 26s/step - accuracy: 0.3750 - loss: 2.2126","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1727796051.521339     836 tpu_compile_op_common.cc:245] Compilation of 930313c919672ecc:0:0 with session name  took 18.489209647s and succeeded\nI0000 00:00:1727796051.591035     836 tpu_compilation_cache_interface.cc:475] TPU host compilation cache: compilation complete for cache_key(930313c919672ecc:0:0), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_iterator_12289784859012943331\", property.function_library_fingerprint = 3516629536828988579, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1727796051.591086     836 tpu_compilation_cache_interface.cc:541] After adding entry for key 930313c919672ecc:0:0 with session_name  cache is 1 entries (69365611 bytes),  marked for eviction 0 entries (0 bytes).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m148/695\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 37ms/step - accuracy: 0.6604 - loss: 1.0941","output_type":"stream"}]},{"cell_type":"code","source":"with tpu_strategy.scope():\n    model.evaluate(test_ds)","metadata":{"execution":{"iopub.status.busy":"2024-09-28T19:11:01.257189Z","iopub.execute_input":"2024-09-28T19:11:01.258248Z","iopub.status.idle":"2024-09-28T19:11:16.559264Z","shell.execute_reply.started":"2024-09-28T19:11:01.258207Z","shell.execute_reply":"2024-09-28T19:11:16.558266Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 37ms/step - accuracy: 0.9556 - loss: 0.2107\n","output_type":"stream"}]},{"cell_type":"code","source":"with tpu_strategy.scope():\n    model.save(\"keras_base_{0}_{1}.h5\".format(condition_for_training, vertebrae_position))","metadata":{"execution":{"iopub.status.busy":"2024-09-28T19:11:21.432933Z","iopub.execute_input":"2024-09-28T19:11:21.433710Z","iopub.status.idle":"2024-09-28T19:11:22.116286Z","shell.execute_reply.started":"2024-09-28T19:11:21.433675Z","shell.execute_reply":"2024-09-28T19:11:22.115184Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}