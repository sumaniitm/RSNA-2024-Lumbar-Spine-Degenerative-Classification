{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":71549,"databundleVersionId":8561470,"sourceType":"competition"},{"sourceId":109574,"sourceType":"modelInstanceVersion","modelInstanceId":91762,"modelId":115977},{"sourceId":109139,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":83858,"modelId":108105},{"sourceId":109575,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":91763,"modelId":115978},{"sourceId":109576,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":91764,"modelId":115979},{"sourceId":109577,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":91765,"modelId":115980}],"dockerImageVersionId":30761,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport glob\nimport os\nimport polars as pl\n# import duckdb as dd\nfrom tqdm import tqdm\nfrom itertools import product\n\"\"\"import matplotlib.pyplot as plt\nimport cv2\nfrom pydicom import dcmread\nimport warnings\nfrom sklearn.preprocessing import LabelEncoder\nimport pickle\nimport gc\nimport ctypes\"\"\"\n# from sklearn.model_selection import train_test_split\nimport tensorflow as tf\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\nimport tensorflow_io as tfio\nfrom tensorflow import keras\nfrom tensorflow.python.keras import backend as K\nfrom joblib import Parallel, delayed","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-22T14:30:52.475812Z","iopub.execute_input":"2024-09-22T14:30:52.476247Z","iopub.status.idle":"2024-09-22T14:31:07.521452Z","shell.execute_reply.started":"2024-09-22T14:30:52.476196Z","shell.execute_reply":"2024-09-22T14:31:07.520398Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"\"\"\"try: # detect TPUs\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.TPUStrategy(tpu)\nexcept ValueError: # detect GPUs\n    strategy = tf.distribute.MirroredStrategy() # for GPU or multi-GPU machines\n    \nprint(\"Number of accelerators: \", strategy.num_replicas_in_sync)\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def label_encoder(label):\n    if label == 'Normal/Mild':\n        return 2\n    elif label == 'Severe':\n        return 3\n    else:\n        return 1\n    \ndef attach_weights(label):\n    if label == 'Normal/Mild':\n        return 1\n    elif label == 'Severe':\n        return 4\n    else:\n        return 2\n    \ndef get_condition(full_location: str) -> str:\n    # Given an input like spinal_canal_stenosis_l1_l2 extracts 'spinal'\n    for injury_condition in ['spinal', 'foraminal', 'subarticular']:\n        if injury_condition in full_location:\n            return injury_condition\n    raise ValueError(f'condition not found in {full_location}')","metadata":{"execution":{"iopub.status.busy":"2024-09-22T14:31:10.451293Z","iopub.execute_input":"2024-09-22T14:31:10.451960Z","iopub.status.idle":"2024-09-22T14:31:10.459401Z","shell.execute_reply.started":"2024-09-22T14:31:10.451918Z","shell.execute_reply":"2024-09-22T14:31:10.458215Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"Test = False\nconfig = {}\n\nif Test:\n    config['root_file_path'] = '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_images'\n    config['start'] = 10\n    config['end'] = 60\n    \n    train_studies_metadata_file_path = '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train.csv'\n    train_studies_metadata_df = pl.read_csv(train_studies_metadata_file_path, low_memory=True)\n    print(\"before dropping nulls :\", train_studies_metadata_df.shape)\n    train_studies_metadata_df = train_studies_metadata_df.drop_nulls()\n    print(\"after dropping nulls :\", train_studies_metadata_df.shape)\n\n    studies_full = train_studies_metadata_df.select(pl.col('study_id')).unique().to_series().to_list()\n    print(\"total number of studies : \", len(studies_full))\n    \n    studies = studies_full[config['start']:config['end']]\n    #studies = os.listdir(config['root_file_path'])\n    test_dict = {}\nelse:\n    config['root_file_path'] = '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/test_images/'\n    studies = os.listdir(config['root_file_path'])\n    test_dict = {}\n    \nfor study in studies:\n    image_files = []\n    for dirname, _, filenames in os.walk(config['root_file_path']+'/'+str(study)):\n        for filename in filenames:\n            test_dict[os.path.join(dirname, filename).split('/')[-3]] = image_files\n            image_files.append(os.path.join(dirname, filename))\n            \nprint(len(test_dict))","metadata":{"execution":{"iopub.status.busy":"2024-09-22T14:31:31.153280Z","iopub.execute_input":"2024-09-22T14:31:31.153717Z","iopub.status.idle":"2024-09-22T14:31:32.287666Z","shell.execute_reply.started":"2024-09-22T14:31:31.153676Z","shell.execute_reply":"2024-09-22T14:31:32.286216Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"before dropping nulls : (1975, 26)\nafter dropping nulls : (1790, 26)\ntotal number of studies :  1790\n50\n","output_type":"stream"}]},{"cell_type":"code","source":"def create_solution_df(run_config, run_test_dict):\n\n    print(\"total number of run_test_dict items : \",len(run_test_dict))\n    \n    train_studies_metadata_df_up = train_studies_metadata_df.unpivot(index=\"study_id\")\n    train_studies_metadata_df_up.columns = ['study_id', 'condition', 'severity']\n\n    train_studies_metadata_df_up = train_studies_metadata_df_up.with_columns([\n        pl.col(\"severity\").map_elements(label_encoder, return_dtype=pl.Int32).alias(\"encoded_severity\"),\n        pl.col(\"severity\").map_elements(attach_weights, return_dtype=pl.Int32).alias(\"sample_weight\"),\n        (pl.col(\"study_id\").cast(pl.String)+'_'+pl.col(\"condition\")).alias(\"row_id\")\n    ])\n\n    print(\"train_studies_metadata_df_up shape : \",train_studies_metadata_df_up.shape)\n    \n    temp = train_studies_metadata_df_up.select([pl.col('study_id'), pl.col('row_id'), pl.col('encoded_severity'), pl.col('severity'), pl.col('sample_weight')])\n    train_studies_metadata_df_final = temp.pivot(\"severity\", index=[\"study_id\",\"row_id\"], values=\"encoded_severity\")\n    train_studies_metadata_df_final.columns = ['study_id', 'row_id', 'normal_mild', 'moderate', 'severe']\n    \n    train_studies_metadata_df_final_2 = train_studies_metadata_df_final.join(temp, on=[\"study_id\",\"row_id\"], how=\"inner\")\n    train_studies_metadata_df_final_2 = train_studies_metadata_df_final_2.drop(['encoded_severity', 'severity'])\n    train_studies_metadata_df_final_2 = train_studies_metadata_df_final_2.with_columns([\n        pl.when(pl.col('normal_mild').is_not_null()).then(1).otherwise(0).alias('true_normal_mild'),\n        pl.when(pl.col('moderate').is_not_null()).then(1).otherwise(0).alias('true_moderate'),\n        pl.when(pl.col('severe').is_not_null()).then(1).otherwise(0).alias('true_severe'),\n    ])\n    \n    train_studies_metadata_df_final_2 = train_studies_metadata_df_final_2.drop(['normal_mild', 'moderate', 'severe'])\n    train_studies_metadata_df_final_2.columns = ['study_id', 'row_id', 'sample_weight', 'normal_mild', 'moderate', 'severe']\n    \n    solutions = train_studies_metadata_df_final_2.filter(pl.col('study_id').is_in(studies))\n    solutions = solutions.drop(['study_id'])\n    print(\"shape of solutions dataframe : \", solutions.shape)\n    \n    return solutions.to_pandas()","metadata":{"execution":{"iopub.status.busy":"2024-09-22T14:31:36.283646Z","iopub.execute_input":"2024-09-22T14:31:36.284091Z","iopub.status.idle":"2024-09-22T14:31:36.295873Z","shell.execute_reply.started":"2024-09-22T14:31:36.284047Z","shell.execute_reply":"2024-09-22T14:31:36.294663Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import log_loss\n\ndef calculate_final_score(solution_df, submission_df):\n    \n    target_levels = ['normal_mild', 'moderate', 'severe']\n\n    if not pd.api.types.is_numeric_dtype(submission_df[target_levels].values):\n            raise ParticipantVisibleError('All submission_df values must be numeric')\n\n    if not np.isfinite(submission_df[target_levels].values).all():\n        raise ParticipantVisibleError('All submission_df values must be finite')\n\n    if solution_df[target_levels].min().min() < 0:\n        raise ParticipantVisibleError('All labels must be at least zero')\n    if submission_df[target_levels].min().min() < 0:\n        raise ParticipantVisibleError('All predictions must be at least zero')\n        \n    solution_df['study_id'] = solution_df['row_id'].apply(lambda x: x.split('_')[0])\n    solution_df['location'] = solution_df['row_id'].apply(lambda x: '_'.join(x.split('_')[1:]))\n    solution_df['condition'] = solution_df['row_id'].apply(get_condition)\n    \n    row_id_column_name = 'row_id'\n\n    del solution_df[row_id_column_name]\n    del submission_df[row_id_column_name]\n    assert sorted(submission_df.columns) == sorted(target_levels)\n\n    submission_df['study_id'] = solution_df['study_id']\n    submission_df['location'] = solution_df['location']\n    submission_df['condition'] = solution_df['condition']\n    \n    condition_losses = []\n    condition_weights = []\n    \n    for condition in ['spinal', 'foraminal', 'subarticular']:\n        condition_indices = solution_df.loc[solution_df['condition'] == condition].index.values\n        condition_loss = log_loss(\n            y_true=solution_df.loc[condition_indices, target_levels].values,\n            y_pred=submission_df.loc[condition_indices, target_levels].values,\n            sample_weight=solution_df.loc[condition_indices, 'sample_weight'].values\n        )\n        condition_losses.append(condition_loss)\n        condition_weights.append(1)\n        \n    any_severe_spinal_labels = pd.Series(solution_df.loc[solution_df['condition'] == 'spinal'].groupby('study_id')['severe'].max())\n    any_severe_spinal_weights = pd.Series(solution_df.loc[solution_df['condition'] == 'spinal'].groupby('study_id')['sample_weight'].max())\n    any_severe_spinal_predictions = pd.Series(submission_df.loc[submission_df['condition'] == 'spinal'].groupby('study_id')['severe'].max())\n    \n    any_severe_scalar = 1.0\n\n    any_severe_spinal_loss = log_loss(\n        y_true=any_severe_spinal_labels,\n        y_pred=any_severe_spinal_predictions,\n        sample_weight=any_severe_spinal_weights\n    )\n    condition_losses.append(any_severe_spinal_loss)\n    condition_weights.append(any_severe_scalar)\n\n    print(\"final score during training : \", np.average(condition_losses, weights=condition_weights))\n    \n    return np.average(condition_losses, weights=condition_weights)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T14:31:42.989245Z","iopub.execute_input":"2024-09-22T14:31:42.989657Z","iopub.status.idle":"2024-09-22T14:31:43.499249Z","shell.execute_reply.started":"2024-09-22T14:31:42.989620Z","shell.execute_reply":"2024-09-22T14:31:43.497935Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"if Test:\n    solution_data = create_solution_df(config, test_dict)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T14:31:48.651544Z","iopub.execute_input":"2024-09-22T14:31:48.652365Z","iopub.status.idle":"2024-09-22T14:31:48.825780Z","shell.execute_reply.started":"2024-09-22T14:31:48.652319Z","shell.execute_reply":"2024-09-22T14:31:48.823860Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"total number of run_test_dict items :  50\ntrain_studies_metadata_df_up shape :  (44750, 6)\nshape of solutions dataframe :  (1250, 5)\n","output_type":"stream"}]},{"cell_type":"code","source":"from multiprocessing import cpu_count\nn_cores = cpu_count()\nprint(f'Number of Logical CPU cores: {n_cores}')","metadata":{"execution":{"iopub.status.busy":"2024-09-22T14:31:59.518492Z","iopub.execute_input":"2024-09-22T14:31:59.518948Z","iopub.status.idle":"2024-09-22T14:31:59.524904Z","shell.execute_reply.started":"2024-09-22T14:31:59.518908Z","shell.execute_reply":"2024-09-22T14:31:59.523647Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Number of Logical CPU cores: 4\n","output_type":"stream"}]},{"cell_type":"code","source":"model_dict = {}\n\n\"\"\"model_dict['right_neural_foraminal_narrowing_l1_l2'] = keras.models.\\\nload_model(\"/kaggle/input/keras_base_right_neural_foraminal_narrowing_l1_l2/tensorflow2/default/1/keras_base_right_neural_foraminal_narrowing_l1_l2.h5\")\n\nmodel_dict['right_neural_foraminal_narrowing_l2_l3'] = keras.models.\\\nload_model(\"/kaggle/input/keras_base_right_neural_foraminal_narrowing_l2_l3/tensorflow2/default/1/keras_base_right_neural_foraminal_narrowing_l2_l3.h5\")\n\nmodel_dict['right_neural_foraminal_narrowing_l3_l4'] = keras.models.\\\nload_model(\"/kaggle/input/keras_base_right_neural_foraminal_narrowing_l3_l4/tensorflow2/default/1/keras_base_right_neural_foraminal_narrowing_l3_l4.h5\")\n\nmodel_dict['right_neural_foraminal_narrowing_l4_l5'] = keras.models.\\\nload_model(\"/kaggle/input/keras_base_right_neural_foraminal_narrowing_l4_l5/tensorflow2/default/1/keras_base_right_neural_foraminal_narrowing_l4_l5.h5\")\n\nmodel_dict['right_neural_foraminal_narrowing_l5_s1'] = keras.models.\\\nload_model(\"/kaggle/input/keras_base_right_neural_foraminal_narrowing_l5_s1/tensorflow2/default/1/keras_base_right_neural_foraminal_narrowing_l5_s1.h5\")\n\nmodel_dict['spinal_canal_stenosis_l1_l2'] = keras.models.\\\nload_model(\"/kaggle/input/keras_base_scs_l1_l2/tensorflow2/default/2/keras_base_spinal_canal_stenosis_l1_l2.h5\")\n\nmodel_dict['spinal_canal_stenosis_l2_l3'] = keras.models.\\\nload_model(\"/kaggle/input/keras_base_spinal_canal_stenosis_l2_l3/tensorflow2/default/1/keras_base_spinal_canal_stenosis_l2_l3.h5\")\n\nmodel_dict['spinal_canal_stenosis_l3_l4'] = keras.models.\\\nload_model(\"/kaggle/input/keras_base_spinal_canal_stenosis_l3_l4/tensorflow2/default/1/keras_base_spinal_canal_stenosis_l3_l4.h5\")\n\nmodel_dict['spinal_canal_stenosis_l4_l5'] = keras.models.\\\nload_model(\"/kaggle/input/keras_base_spinal_canal_stenosis_l4_l5/tensorflow2/default/1/keras_base_spinal_canal_stenosis_l4_l5.h5\")\"\"\"\n\nmodel_dict['spinal_canal_stenosis_l5_s1'] = keras.models.\\\nload_model(\"/kaggle/input/keras_base_spinal_canal_stenosis_l5_s1/tensorflow2/default/2/keras_base_spinal_canal_stenosis_l5_s1.h5\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T14:32:06.425291Z","iopub.execute_input":"2024-09-22T14:32:06.425739Z","iopub.status.idle":"2024-09-22T14:32:09.311717Z","shell.execute_reply.started":"2024-09-22T14:32:06.425697Z","shell.execute_reply":"2024-09-22T14:32:09.310604Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def read_and_parse_dicom_files_for_inf(full_file_path):\n    tf.config.run_functions_eagerly(True)\n    raw_image = tf.io.read_file(full_file_path)\n    sp = tf.strings.split(tf.gather(tf.strings.split(full_file_path, 'images/'), 1), '/')\n    N = tf.size(sp)\n    LEN = tf.strings.length(tf.gather(sp, 0))+tf.strings.length(tf.gather(sp, 2))\n    \n    # Add missing file metadata to avoid warnnigs flooding\n    if   LEN==12: raw_image = tf.strings.regex_replace(raw_image, pattern=b'DICM\\x02\\x00\\x01\\x00', rewrite=b'DICM\\x02\\x00\\x00\\x00UL\\x04\\x00\\x92\\x00\\x00\\x00\\x02\\x00\\x01\\x00')\n    elif LEN==13: raw_image = tf.strings.regex_replace(raw_image, pattern=b'DICM\\x02\\x00\\x01\\x00', rewrite=b'DICM\\x02\\x00\\x00\\x00UL\\x04\\x00\\x92\\x00\\x00\\x00\\x02\\x00\\x01\\x00')\n    elif LEN==14: raw_image = tf.strings.regex_replace(raw_image, pattern=b'DICM\\x02\\x00\\x01\\x00', rewrite=b'DICM\\x02\\x00\\x00\\x00UL\\x04\\x00\\x94\\x00\\x00\\x00\\x02\\x00\\x01\\x00')\n    elif LEN==15: raw_image = tf.strings.regex_replace(raw_image, pattern=b'DICM\\x02\\x00\\x01\\x00', rewrite=b'DICM\\x02\\x00\\x00\\x00UL\\x04\\x00\\x94\\x00\\x00\\x00\\x02\\x00\\x01\\x00')\n    elif LEN==16: raw_image = tf.strings.regex_replace(raw_image, pattern=b'DICM\\x02\\x00\\x01\\x00', rewrite=b'DICM\\x02\\x00\\x00\\x00UL\\x04\\x00\\x96\\x00\\x00\\x00\\x02\\x00\\x01\\x00')\n    elif LEN==17: raw_image = tf.strings.regex_replace(raw_image, pattern=b'DICM\\x02\\x00\\x01\\x00', rewrite=b'DICM\\x02\\x00\\x00\\x00UL\\x04\\x00\\x96\\x00\\x00\\x00\\x02\\x00\\x01\\x00')\n    elif LEN==18: raw_image = tf.strings.regex_replace(raw_image, pattern=b'DICM\\x02\\x00\\x01\\x00', rewrite=b'DICM\\x02\\x00\\x00\\x00UL\\x04\\x00\\x98\\x00\\x00\\x00\\x02\\x00\\x01\\x00')\n    \n    #image_bytes = tf.io.read_file(full_file_path)\n    #image = tfio.image.decode_dicom_image(image_bytes, scale='auto', dtype=tf.float32)\n    image = tfio.image.decode_dicom_image(raw_image, scale='auto', dtype=tf.float32)\n    m, M=tf.math.reduce_min(image), tf.math.reduce_max(image)\n    image = (tf.image.grayscale_to_rgb(image)-m)/(M-m)\n    image = tf.image.resize(image, (128,128))\n    return tf.squeeze(image)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T14:32:11.936544Z","iopub.execute_input":"2024-09-22T14:32:11.936983Z","iopub.status.idle":"2024-09-22T14:32:11.948525Z","shell.execute_reply.started":"2024-09-22T14:32:11.936946Z","shell.execute_reply":"2024-09-22T14:32:11.947253Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"vfunc = np.vectorize(read_and_parse_dicom_files_for_inf, otypes=[object])\n\ndef get_predictions(key, model_to_use):\n    final_feature_list = vfunc(test_dict[key]).tolist()\n    final = np.array(final_feature_list)\n    return model_to_use.predict(final)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T14:32:13.222679Z","iopub.execute_input":"2024-09-22T14:32:13.223125Z","iopub.status.idle":"2024-09-22T14:32:13.229560Z","shell.execute_reply.started":"2024-09-22T14:32:13.223084Z","shell.execute_reply":"2024-09-22T14:32:13.228105Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Inference With GPU Support","metadata":{}},{"cell_type":"code","source":"\"\"\"rows = {}\nwith strategy.scope():\n    if Test:\n        for key, value in model_dict.items():\n            print(\"running for key :\", key)\n            y_proba = [get_predictions(st, model_dict[key]) for st in tqdm(test_dict.keys())] ## 27 min with 2 GPUs; not under strategy\n            for i in range(len(y_proba)):\n                rows[list(test_dict.keys())[i]+'_'+key] = np.mean(y_proba[i], axis=0)\n    else:\n        #y_proba = [get_predictions(st, model) for st in test_dict.keys()]\n        for key, value in model_dict.items():\n            y_proba = [get_predictions(st, model_dict[key]) for st in test_dict.keys()] ## 27 min with 2 GPUs; not under strategy\n            for i in range(len(y_proba)):\n                rows[list(test_dict.keys())[i]+'_'+key] = np.mean(y_proba[i], axis=0)\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference w/o GPU support using parallel processing","metadata":{}},{"cell_type":"code","source":"key_combo = product(model_dict.keys(), test_dict.keys())\n\nrows = {}\n\nif Test:\n    y_proba = (Parallel(n_jobs=4)(delayed(get_predictions)(tpl[1], model_dict[tpl[0]]) for tpl in tqdm(key_combo)))\n    for key, value in model_dict.items():\n        for i in range(len(y_proba)):\n                rows[list(test_dict.keys())[i%len(test_dict)]+'_'+key] = np.mean(y_proba[i], axis=0)\nelse:\n    y_proba = (Parallel(n_jobs=4)(delayed(get_predictions)(tpl[1], model_dict[tpl[0]]) for tpl in key_combo))\n    for key, value in model_dict.items():\n        for i in range(len(y_proba)):\n                rows[list(test_dict.keys())[i%len(test_dict)]+'_'+key] = np.mean(y_proba[i], axis=0)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T14:32:26.099409Z","iopub.execute_input":"2024-09-22T14:32:26.099821Z","iopub.status.idle":"2024-09-22T14:35:43.256476Z","shell.execute_reply.started":"2024-09-22T14:32:26.099782Z","shell.execute_reply":"2024-09-22T14:35:43.255184Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"4it [00:00, 30.67it/s]/opt/conda/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adamax', because it has 34 variables whereas the saved optimizer has 2 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\n/opt/conda/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adamax', because it has 34 variables whereas the saved optimizer has 2 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\n/opt/conda/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adamax', because it has 34 variables whereas the saved optimizer has 2 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\n/opt/conda/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adamax', because it has 34 variables whereas the saved optimizer has 2 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\n/opt/conda/lib/python3.10/site-packages/numpy/lib/function_base.py:2455: RuntimeWarning: invalid value encountered in read_and_parse_dicom_files_for_inf (vectorized)\n  outputs = ufunc(*inputs)\n/opt/conda/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/numpy/lib/function_base.py:2455: RuntimeWarning: invalid value encountered in read_and_parse_dicom_files_for_inf (vectorized)\n  outputs = ufunc(*inputs)\n/opt/conda/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n  warnings.warn(\n4it [00:19, 30.67it/s]/opt/conda/lib/python3.10/site-packages/numpy/lib/function_base.py:2455: RuntimeWarning: invalid value encountered in read_and_parse_dicom_files_for_inf (vectorized)\n  outputs = ufunc(*inputs)\n/opt/conda/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3s/step","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/numpy/lib/function_base.py:2455: RuntimeWarning: invalid value encountered in read_and_parse_dicom_files_for_inf (vectorized)\n  outputs = ufunc(*inputs)\n/opt/conda/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n  warnings.warn(\n8it [00:23,  3.41s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step\n\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2s/step","output_type":"stream"},{"name":"stderr","text":"12it [00:36,  3.42s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step\n\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2s/step","output_type":"stream"},{"name":"stderr","text":"16it [00:50,  3.43s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step\n\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2s/step","output_type":"stream"},{"name":"stderr","text":"20it [01:05,  3.52s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 827ms/step\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 980ms/step\n\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2s/step","output_type":"stream"},{"name":"stderr","text":"24it [01:19,  3.54s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step\n","output_type":"stream"},{"name":"stderr","text":"28it [01:34,  3.57s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 789ms/step\n\u001b[1m2/3\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2s/step","output_type":"stream"},{"name":"stderr","text":"32it [01:46,  3.41s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step\n","output_type":"stream"},{"name":"stderr","text":"36it [02:00,  3.43s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step\n","output_type":"stream"},{"name":"stderr","text":"40it [02:14,  3.44s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2s/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step\n\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2s/step","output_type":"stream"},{"name":"stderr","text":"44it [02:33,  3.88s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2s/step\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step\n\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2s/step","output_type":"stream"},{"name":"stderr","text":"50it [02:47,  3.36s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step\n\u001b[1m2/3\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2s/step","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 978ms/step\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2s/step\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 688ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/sample_submission.csv')\nsubmission['row_id'] = 'samples'","metadata":{"execution":{"iopub.status.busy":"2024-09-22T14:36:46.722910Z","iopub.execute_input":"2024-09-22T14:36:46.723414Z","iopub.status.idle":"2024-09-22T14:36:46.749627Z","shell.execute_reply.started":"2024-09-22T14:36:46.723357Z","shell.execute_reply":"2024-09-22T14:36:46.748308Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### Create a dictionary based on all combinations and whether models are available for them or not","metadata":{}},{"cell_type":"code","source":"conditions = ['spinal_canal_stenosis', 'neural_foraminal_narrowing', 'subarticular_stenosis']\nsides = ['left', 'right']\nvertebrae_levels = ['l1_l2', 'l2_l3', 'l3_l4', 'l4_l5', 'l5_s1']\n# severity_levels = ['normal_mild', 'moderate', 'severe']\nseverity_levels = ['moderate', 'normal_mild', 'severe']\n\ncondn_sides_vrtlvl_combos = product(conditions, sides, vertebrae_levels)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T14:36:50.057748Z","iopub.execute_input":"2024-09-22T14:36:50.058201Z","iopub.status.idle":"2024-09-22T14:36:50.065015Z","shell.execute_reply.started":"2024-09-22T14:36:50.058153Z","shell.execute_reply":"2024-09-22T14:36:50.063579Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"combinations = {}\n\nfor i in condn_sides_vrtlvl_combos:\n    #print(i)\n    if i[0] != 'spinal_canal_stenosis':\n        combinations[i[1]+'_'+i[0]+'_'+i[2]] = 'N'\n    else:\n        if i[2] == 'l5_s1':\n            combinations[i[0]+'_'+i[2]] = 'Y'\n        else:\n            combinations[i[0]+'_'+i[2]] = 'N'\n            \n#print(combinations)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T14:36:56.274622Z","iopub.execute_input":"2024-09-22T14:36:56.275072Z","iopub.status.idle":"2024-09-22T14:36:56.282580Z","shell.execute_reply.started":"2024-09-22T14:36:56.275030Z","shell.execute_reply":"2024-09-22T14:36:56.281104Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"for st in test_dict.keys():\n    for key, value in combinations.items():\n        if value == 'Y':\n            pass\n        else:\n            rows[st+'_'+key] = np.array([0.333333, 0.333333, 0.333333])","metadata":{"execution":{"iopub.status.busy":"2024-09-22T14:37:00.338549Z","iopub.execute_input":"2024-09-22T14:37:00.338974Z","iopub.status.idle":"2024-09-22T14:37:00.346961Z","shell.execute_reply.started":"2024-09-22T14:37:00.338933Z","shell.execute_reply":"2024-09-22T14:37:00.345628Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# weight_dict = {'normal_mild':1, 'moderate':2, 'severe':4}\n\"\"\"conditions = ['spinal_canal_stenosis', 'neural_foraminal_narrowing', 'subarticular_stenosis']\nsides = ['left', 'right']\nvertebrae_levels = ['l1_l2', 'l2_l3', 'l3_l4', 'l4_l5', 'l5_s1']\nseverity_levels = ['normal_mild', 'moderate', 'severe']\n\nfor c in conditions:\n    for v in vertebrae_levels:\n        if c != 'spinal_canal_stenosis':\n            for s in sides:\n                if s+'_'+c != 'right_neural_foraminal_narrowing':\n                    for st in test_dict.keys():\n                        rows[st+'_'+s+'_'+c+'_'+v] = np.array([0.333333, 0.333333, 0.333333])\n                else:\n                    pass\n        else:\n            pass\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-09-21T12:53:05.863368Z","iopub.execute_input":"2024-09-21T12:53:05.863853Z","iopub.status.idle":"2024-09-21T12:53:05.889523Z","shell.execute_reply.started":"2024-09-21T12:53:05.863807Z","shell.execute_reply":"2024-09-21T12:53:05.887969Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"if Test:\n    for row_id, feature in tqdm(rows.items()):\n        feature_set_reshaped = feature.reshape(1, -1)\n        predictions = np.ascontiguousarray(feature_set_reshaped)\n        df = pd.DataFrame(predictions, columns=severity_levels)\n        df.insert(loc=0, column='row_id', value=row_id)\n        submission = pd.concat([submission,df]).reset_index(drop=True)\n\n    i = submission[(submission.row_id == 'samples')].index\n    submission = submission.drop(i).reset_index(drop=True)\nelse:\n    for row_id, feature in rows.items():\n        feature_set_reshaped = feature.reshape(1, -1)\n        predictions = np.ascontiguousarray(feature_set_reshaped)\n        df = pd.DataFrame(predictions, columns=severity_levels)\n        df.insert(loc=0, column='row_id', value=row_id)\n        submission = pd.concat([submission,df]).reset_index(drop=True)\n\n    i = submission[(submission.row_id == 'samples')].index\n    submission = submission.drop(i).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T14:37:09.479485Z","iopub.execute_input":"2024-09-22T14:37:09.479925Z","iopub.status.idle":"2024-09-22T14:37:11.267224Z","shell.execute_reply.started":"2024-09-22T14:37:09.479883Z","shell.execute_reply":"2024-09-22T14:37:11.266068Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"100%|██████████| 1250/1250 [00:01<00:00, 706.68it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T05:13:33.132493Z","iopub.execute_input":"2024-09-22T05:13:33.133849Z","iopub.status.idle":"2024-09-22T05:13:33.171414Z","shell.execute_reply.started":"2024-09-22T05:13:33.133791Z","shell.execute_reply":"2024-09-22T05:13:33.169908Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"if Test:\n    calculate_final_score(solution_data, submission)\n    print(set(solution_data['location'] == submission['location']))","metadata":{"execution":{"iopub.status.busy":"2024-09-22T14:37:19.630237Z","iopub.execute_input":"2024-09-22T14:37:19.630677Z","iopub.status.idle":"2024-09-22T14:37:19.689217Z","shell.execute_reply.started":"2024-09-22T14:37:19.630628Z","shell.execute_reply":"2024-09-22T14:37:19.688057Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"final score during training :  0.9314214464875812\n{True}\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}